---
title: 'Text mining, sentiment analysis, and visualization'
date: 'created on 22 November 2020 and updated `r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

```

## Get GOT text
```{r get-document}
Got_path <- here("data/got.pdf")
Got_path
Got_text <- pdf_text(Got_path)
```

Example: Just want to get text from a single page (e.g. Page 9)? 
```{r single-page}
Got_p9 <- Got_text[9]
Got_p9
```

### Some wrangling:

- Split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- Unnest into regular columns using `tidyr::unnest()`
- Remove leading/trailing white space with `stringr::str_trim()`

```{r split-lines}
Got_df <- data.frame(Got_text) %>% 
  mutate(text_full = str_split(Got_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full))

Got_df 
```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Get the tokens (individual words) in tidy format

Use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. We are interested in *words*, so that's the token we'll use:

```{r tokenize}
Got_tokens <- Got_df %>% 
  unnest_tokens(word, text_full)
Got_tokens
```

Let's count the words!
```{r count-words}
Got_wc <- Got_tokens %>% 
  count(word) %>% 
  arrange(-n)
Got_wc
```


### Remove stop words:

See `?stop_words` and `View(stop_words)`to look at documentation for stop words lexicons.

We will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
view(stop_words)
stop_words

Got_stop <- Got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-Got_text)
```

Now check the counts again: 
```{r count-words2}
Got_swc <- Got_stop %>% 
  count(word) %>% 
  arrange(-n)
Got_swc
```

What if we want to get rid of all the numbers (non-text) in `Got_stop`?
```{r skip-numbers}
Got_no_numeric <- Got_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of IPCC report words (non-numeric)


```{r wordcloud-prep}
# length(unique(Got_no_numeric$word))

#the top 100 most frequent
Got_top100 <- Got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
Got_top100
```

```{r wordcloud}
Got_cloud <- ggplot(data = Got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

Got_cloud
```

Customize!:
```{r wordcloud-pro}
ggplot(data = Got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "star") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

### Sentiment analysis

"afinn": Words ranked from -5 (very negative) to +5 (very positive)
```{r afinn}
get_sentiments(lexicon = "afinn")

#Positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))
afinn_pos
```

bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```

Includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative. 

Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```

Let's do sentiment analysis on the Got_text data using afinn, and nrc. 


### Sentiment analysis with afinn: 

First, bind words in `ipcc_stop` to `afinn` lexicon:
```{r bind-afinn}
Got_afinn <- Got_stop %>% 
  inner_join(get_sentiments("afinn"))
Got_afinn
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
Got_afinn_hist <- Got_afinn %>% 
  count(value)

ggplot(data = Got_afinn_hist, aes(x = value, y = n)) +
  geom_col(aes(fill = value)) +
  theme_bw()
```

Investigate some of the words in a bit more depth:
```{r afinn-2}
Got_afinn2 <- Got_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}
unique(Got_afinn2$word)

Got_afinn2_n <- Got_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n)) %>% 
  slice(1:20)

ggplot(data = Got_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip() +
  theme_bw()
```

summarize sentiment for the report: 
```{r summarize-afinn}
Got_summary <- Got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
Got_summary
  
```

The mean and median indicate negative overall sentiments based on the AFINN lexicon. 

### NRC lexicon for sentiment analysis

```{r bind-bing}
Got_nrc <- Got_stop %>% 
  inner_join(get_sentiments("nrc")) 

```
 check which are excluded using `anti_join()`:

```{r check-exclusions}
Got_exclude <- Got_stop %>% 
  anti_join(get_sentiments("nrc"))

View(Got_exclude)

# Count to find the most excluded:
Got_exclude_n <- Got_exclude %>% 
  count(word, sort = TRUE)

head(Got_exclude_n)
```

**Lesson: always check which words are EXCLUDED in sentiment analysis using a pre-built lexicon! **

Now find some counts: 
```{r count-bing}
Got_nrc_n <- Got_nrc %>% 
  count(sentiment, sort = TRUE)
ggplot(data = Got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()
```

Or count by sentiment *and* word, then facet:
```{r count-nrc}
Got_nrc_n5 <- Got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

Got_nrc_n5

Got_nrc_gg <- ggplot(data = Got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

Got_nrc_gg

ggsave(plot = Got_nrc_gg, 
       here("figures","Got_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```

Wait, so "confidence" is showing up in NRC lexicon as "fear"? Let's check:
```{r nrc-confidence}
leave <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "leave")
leave
```

## Your task

Taking this script as a point of departure, apply sentiment analysis on the Game of Thrones. You will find a pdf in the data folder. What are the most common meaningful words and what emotions do you expect will dominate this volume? Are there any terms that are similarly ambiguous to the 'confidence' above? 

The most meaningful words in GOT using the 10 categories are: 
Anger - Stone 
Disgust - lord 
Joy - found 
Positive - lord 
Surprise - leave 
Anticipation - Time 
Fear - watch 
Negative - lord 
Sadness - dark 
Trust - lord 
I have not read or seen any of the GOT books/movies, but from what i know there are some lords in it, so lords showing up here makes sense as far as i am concerned.
There are a lot of ambiguous words. Some of these words fit in the same category. 


